{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "Delelop language model, which generates death metal band names.  \n",
    "You can get data from https://www.kaggle.com/zhangjuefei/death-metal.  \n",
    "You are free to use any other data, but the most easy way is just to take the band name column.\n",
    "\n",
    "Your language model should be char-based autogression RNN.  \n",
    "Text generation should be terminated when either max length is reached or terminal symbol is generated.  \n",
    "\n",
    "<img src=\"images/example.png\">\n",
    "\n",
    "<img src=\"images/example2.png\">\n",
    "\n",
    "Different band names can be generated by:  \n",
    "1. init $h_0$ as random vector from some probabilty distribution.\n",
    "2. sampling over tokens at each timestep with probability = softmax \n",
    "\n",
    "Calculate perplexity for your model = your objective quality metric.  \n",
    "Also, sample 10 band names from your model for subjective evaluation. E.g. names like 'qwiouefiou23riop2h3' or 'death death death!' are bad examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset, BPTTIterator\n",
    "from torch.distributions.distribution import Distribution\n",
    "#from tqdm import tqdm_notebook\n",
    "from tqdm.autonotebook import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "SEED = 42\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bands.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>status</th>\n",
       "      <th>formed_in</th>\n",
       "      <th>genre</th>\n",
       "      <th>theme</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>('M') Inc.</td>\n",
       "      <td>United States</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>(sic)</td>\n",
       "      <td>United States</td>\n",
       "      <td>Split-up</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993-1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.F.O.A.D.</td>\n",
       "      <td>France</td>\n",
       "      <td>Active</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>Life and Death</td>\n",
       "      <td>2009-present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100 Suns</td>\n",
       "      <td>United States</td>\n",
       "      <td>Active</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004-present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 Days of Anarchy</td>\n",
       "      <td>United States</td>\n",
       "      <td>Split-up</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Death Metal</td>\n",
       "      <td>Anarchy</td>\n",
       "      <td>1998-2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                name        country    status  formed_in        genre  \\\n",
       "0   1          ('M') Inc.  United States   Unknown     2009.0  Death Metal   \n",
       "1   2               (sic)  United States  Split-up     1993.0  Death Metal   \n",
       "2   3           .F.O.A.D.         France    Active     2009.0  Death Metal   \n",
       "3   4            100 Suns  United States    Active     2004.0  Death Metal   \n",
       "4   5  12 Days of Anarchy  United States  Split-up     1998.0  Death Metal   \n",
       "\n",
       "            theme        active  \n",
       "0             NaN        2009-?  \n",
       "1             NaN     1993-1996  \n",
       "2  Life and Death  2009-present  \n",
       "3             NaN  2004-present  \n",
       "4         Anarchy     1998-2002  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37723, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t = list(data['name'])\n",
    "shuffle(data_t)\n",
    "\n",
    "with open('bands.txt', 'a') as f:\n",
    "    for t in data_t:\n",
    "        f.write(t + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bands.txt', 'r') as file, open('train.txt', 'a') as file_w1, open('val.txt', 'a') as file_w2:\n",
    "    lines = file.readlines()\n",
    "    sep = int(0.9 * len(lines))\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "        if i < sep:\n",
    "            file_w1.write(line)\n",
    "        else:\n",
    "            file_w2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bands.txt', 'r') as f:\n",
    "    with open('train.txt', 'a') as f_tr:\n",
    "        with open('val.txt', 'a') as f_val:\n",
    "            ln = f.readlines()\n",
    "            i = 0\n",
    "            mark = int(0.9 * len(ln))\n",
    "            \n",
    "            for line in ln:\n",
    "                i += 1\n",
    "                if i < mark:\n",
    "                    f_tr.write(line)\n",
    "                else:\n",
    "                    f_val.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        encoded = self.encoder(input)\n",
    "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        return Variable(tt.zeros(self.n_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ch = string.printable\n",
    "char_len = len(all_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = tt.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        try:\n",
    "            tensor[c] = all_ch.index(string[c])\n",
    "        except:\n",
    "            continue\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def random_training_set(chunk_len, batch_size, file, file_len):\n",
    "    \n",
    "    inp = tt.LongTensor(batch_size, chunk_len)\n",
    "    target = tt.LongTensor(batch_size, chunk_len)\n",
    "    \n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, file_len - chunk_len)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = file[start_index:end_index]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "        \n",
    "    inp = Variable(inp)\n",
    "    target = Variable(target)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "\n",
    "def perplexity(x):\n",
    "    return 2**x\n",
    "\n",
    "\n",
    "def _train_epoch(inp, target, model, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    decoder.train()\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    decoder.zero_grad()\n",
    "    running_loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    for c in range(chunk_len):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = decoder(inp[:,c], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "        perplexities.append(perplexity(loss.item()))\n",
    "        \n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        loss_smoothing = c / (c+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "    \n",
    "    PERPLEXITY = np.mean(perplexities)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return running_loss, PERPLEXITY\n",
    "\n",
    "def _test_epoch(inp, target, model, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    with tt.no_grad():\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = decoder(inp[:,c], hidden)\n",
    "            loss = criterion(output.view(batch_size, -1), target[:,c])\n",
    "            perplexities.append(perplexity(loss.item()))\n",
    "            epoch_loss += loss.data.item()\n",
    "    PERPLEXITY = np.mean(perplexities)\n",
    "    \n",
    "    return epoch_loss / chunk_len, PERPLEXITY\n",
    "\n",
    "\n",
    "def nn_train(model, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss, train_per = _train_epoch(*random_training_set(300, batch_size, file_train, file_train_),\n",
    "                                             model, optimizer, criterion, epoch)\n",
    "        valid_loss, valid_per = _test_epoch(*random_training_set(300, batch_size, file_val, file_val_),\n",
    "                                            model, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        if epoch % 100 == 0 or epoch == n_epochs-1:\n",
    "            print('Epoch %s <--> Valid loss %.5f <--> Train loss %.5f <--> Valid perplexity %.5f <--> Train perplexity %.5f' % (str(epoch),\n",
    "                                                                                                                          valid_loss,\n",
    "                                                                                                                          train_loss,\n",
    "                                                                                                                          valid_per,\n",
    "                                                                                                                          train_per,\n",
    "                                                                                                                          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('train.txt', 'r') as f:\n",
    "    file_train = f.read()\n",
    "file_train_ = len(file_train)\n",
    "\n",
    "\n",
    "with open('val.txt', 'r') as f:\n",
    "    file_val = f.read()\n",
    "file_val_ = len(file_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "batch_size = 32\n",
    "chunk_len = 200\n",
    "\n",
    "decoder = MyModel(len(all_ch),\n",
    "                  hidden_size, \n",
    "                  len(all_ch))\n",
    "\n",
    "optimizer = tt.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 <--> Valid loss 4.43708 <--> Train loss 4.61319 <--> Valid perplexity 21.67876 <--> Train perplexity 24.47978\n",
      "Epoch 100 <--> Valid loss 2.71847 <--> Train loss 2.72899 <--> Valid perplexity 6.65616 <--> Train perplexity 6.73321\n",
      "Epoch 200 <--> Valid loss 2.71121 <--> Train loss 2.76085 <--> Valid perplexity 6.63945 <--> Train perplexity 6.92165\n",
      "Epoch 300 <--> Valid loss 2.74093 <--> Train loss 2.72001 <--> Valid perplexity 6.80755 <--> Train perplexity 6.71317\n",
      "Epoch 400 <--> Valid loss 2.73104 <--> Train loss 2.71503 <--> Valid perplexity 6.75537 <--> Train perplexity 6.66524\n",
      "Epoch 500 <--> Valid loss 2.70650 <--> Train loss 2.69153 <--> Valid perplexity 6.64979 <--> Train perplexity 6.58943\n",
      "Epoch 600 <--> Valid loss 2.66933 <--> Train loss 2.63552 <--> Valid perplexity 6.48999 <--> Train perplexity 6.32989\n",
      "Epoch 700 <--> Valid loss 2.69908 <--> Train loss 2.71832 <--> Valid perplexity 6.62997 <--> Train perplexity 6.72221\n",
      "Epoch 800 <--> Valid loss 2.70679 <--> Train loss 2.69091 <--> Valid perplexity 6.63602 <--> Train perplexity 6.57944\n",
      "Epoch 900 <--> Valid loss 2.69197 <--> Train loss 2.66526 <--> Valid perplexity 6.57029 <--> Train perplexity 6.45992\n",
      "Epoch 999 <--> Valid loss 2.67741 <--> Train loss 2.66769 <--> Valid perplexity 6.52483 <--> Train perplexity 6.46687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type MyModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "nn_train(decoder, criterion, optimizer, n_epochs=1000)\n",
    "tt.save(decoder, 'res.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_names(decoder, prime_str='\\n', predict_len=50, temperature=0.8):\n",
    "\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = char_tensor(prime_str).unsqueeze(0)\n",
    "    predicted = ''\n",
    "\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = tt.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_ch[top_i]\n",
    "        \n",
    "        if predicted and predicted_char == '\\n':\n",
    "            break\n",
    "        else:\n",
    "            predicted += predicted_char\n",
    "            inp = char_tensor(predicted_char).unsqueeze(0)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'res.pt'\n",
    "decoder = tt.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lostuss Cett of forve\n",
      "Cyreecetrous\n",
      "Cidthon Cit\n",
      "Cyric\n",
      "Sagapackavigus\n",
      "Ceroit\n",
      "Cecith\n",
      "Givied\n",
      "Gonionttional tous\n",
      "Siperestion\n",
      "Klof Dpivee\n",
      "Dehan iesos Mereent\n",
      "L0wate of Wheredasspingus\n",
      "Gre Miton Retiied\n",
      "Whityn\n",
      "Llaning untes\n",
      "Dawkiod\n",
      "Morctied\n",
      "Bured Cescienioning\n",
      "Coreig\n",
      "Oriped\n",
      "Lapiit\n",
      "Frive\n",
      "Bulthtion of Rouionion Andis\n",
      "Onfofusissee\n",
      "Lesvid\n",
      "Lortetiund Outhecres\n",
      "Bryn\n",
      "Wabion\n",
      "Jued Cresia\n"
     ]
    }
   ],
   "source": [
    "for x in range(30):\n",
    "    print(group_names(decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
