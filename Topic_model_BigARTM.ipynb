{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основаная задача - **построить хорошую тематическую модель с интерпретируемыми топиками с помощью BigARTM**.\n",
    "\n",
    "1) сделайте нормализацию (если pymorphy2 работает долго используйте mystem или попробуйте установить быструю версию - `pip install pymorphy2[fast]`, можно использовать какой-то другой токенизатор) \n",
    "\n",
    "2) добавьте нграммы (в тетрадке есть закомменченая ячейка с Phrases, можно также попробовать другие способы построить нграммы); \n",
    "\n",
    "3) сохраните тексты .vw формате;\n",
    "\n",
    "4) сделайте хороший словарь (отфильтруйте слишком частотные и редкие слова, попробуйте удалить стоп-слова, сохраните словарь и посмотрите на него, вдруг что-то плохое сразу будет заметно - из словаря можно просто вручную или правилом удалять строки, при загрузке ничего не сломается); \n",
    "\n",
    "5) постройте несколько ARTM моделей (переберите количество тем, поменяйте значения tau у регуляризаторов), если получаются плохие темы, поработайте дополнительно над предобработкой и словарем; \n",
    "\n",
    "6) для самой хорошей модели в отдельной ячейке напечатайте 3 хороших (на ваш вкус) темы\n",
    "\n",
    "7) в другой ячейке нарисуйте график обучения этой модели \n",
    "\n",
    "8) в третьей ячейки опишите какие параметры (количество тем, регуляризаторы, их tau) вы использовали и как обучали (например, после скольки проходов добавили регуляризатор разрежнивания тем (Phi), добавляли ли разреженность документам (Theta) и когда, как повышали значения, сколько итерации модель продожала улучшаться (снижалась перплексия, росли другие метрики);\n",
    "\n",
    "Сохраните тетрадку с экспериментами и положите её на гитхаб, ссылку на неё укажите в форме.\n",
    "\n",
    "**Оцениваться будут главным образом пункты 6, 7 и 8. (3, 1, 4 баллов соответственно). Чтобы заработать остальные 2 балла, нужно хотя бы немного изменить мой код на промежуточных этапах (добавить что-то, указать другие параметры и т.д). **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os, re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import string\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import gensim\n",
    "morph = MorphAnalyzer()\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('russian')) | {'gt',}\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "def normalize(words):\n",
    "    norm_words = [morph.parse(word)[0].normal_form for word in words if len(set(word)) > 1]\n",
    "    return norm_words\n",
    "\n",
    "def opt_normalize(texts, top=None):\n",
    "    uniq = Counter()\n",
    "    for text in texts:\n",
    "        uniq.update(text)\n",
    "    \n",
    "    norm_uniq = {word:morph.parse(word)[0].normal_form for word, _ in uniq.most_common(top)}\n",
    "    \n",
    "    norm_texts = []\n",
    "    for text in texts:\n",
    "        \n",
    "        norm_words = [norm_uniq.get(word) for word in text]\n",
    "        norm_words = [word for word in norm_words if word and word not in stops]\n",
    "        norm_texts.append(norm_words)\n",
    "        \n",
    "    return norm_texts\n",
    "\n",
    "def tokenize(text):\n",
    "    words = [word.strip(string.punctuation) for word in text.split()]\n",
    "    words = [word for word in words if word]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возьмем теже данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habr_texts = opt_normalize([tokenize(remove_tags(text.lower())) for text in open('habr_texts.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вернемся сюда через какое-то время\n",
    "# ph = gensim.models.Phrases(habr_texts, scoring='npmi', threshold=0.3, \n",
    "#                            common_terms=set(stopwords.words('russian'))) # можно указать слова, которые \n",
    "                                                                          # не будут учитываться\n",
    "# p = gensim.models.phrases.Phraser(ph)\n",
    "# ngrammed_habr_texts = p[habr_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p[habr_texts[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для BigARTM требуется специальный формат данных. Их несколько, но мы возьмем vowpal wabbit.  \n",
    "На каждой строчке файла находится одельный текст, записывается такст вот таким образом:  \n",
    "```doc_name |@class_id word_1:1 word_2:3```  \n",
    "\n",
    "|@class_id - задает модальность, но когда она одна её можно не указывать.\n",
    "\n",
    "word_2:3 - слово и его частота (обратите внимание, что : - специальный символ и в словах его быть не может"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('habr_texts.vw', 'w')\n",
    "\n",
    "for i, text in enumerate(habr_texts):\n",
    "    c = Counter(text)\n",
    "    doc = 'doc_'+ str(i) + ' '\n",
    "    vw_text = ' '.join([x.replace(':', '')+':'+str(c[x]) for x in c])\n",
    "    \n",
    "    f.write(doc + vw_text  + '\\n')\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки будем использовать такую функцию (из туториалов от создателей библиотеки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(model_artm):\n",
    "    print('Sparsity Phi: {0:.3f} ARTM)'.format(\n",
    "        model_artm.score_tracker['SparsityPhiScore'].last_value)\n",
    ")\n",
    "    print('Sparsity Theta: {0:.3f} (ARTM)'.format(\n",
    "        model_artm.score_tracker['SparsityThetaScore'].last_value))\n",
    "\n",
    "    print('Kernel contrast: {0:.3f} (ARTM)'.format(\n",
    "        model_artm.score_tracker['TopicKernelScore'].last_average_contrast))\n",
    "\n",
    "    print('Kernel purity: {0:.3f} (ARTM)'.format(\n",
    "        model_artm.score_tracker['TopicKernelScore'].last_average_purity))\n",
    "\n",
    "    print('Perplexity: {0:.3f} (ARTM)'.format(\n",
    "        model_artm.score_tracker['PerplexityScore'].last_value)\n",
    ")\n",
    "    plt.plot(range(model_artm.num_phi_updates), model_artm.score_tracker['PerplexityScore'].value, 'r--', linewidth=2)\n",
    "    plt.xlabel('Iterations count')\n",
    "    plt.ylabel(' ARTM perp. (red)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigARTM работает не с целым файлом, а с кусочками. Поэтому разбиваем наш .vw файл специальным классом в artm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='habr_texts.vw',  # название файла\n",
    "                                        data_format='vowpal_wabbit', # формат файла, у нас vw\n",
    "                                        target_folder='batches', # название папки в которую положаться батчи\n",
    "                                       batch_size=1000) # размер батча, подбирайте под свою память"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже созданные батчи можно заново загружать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='batches', # название папки с батчами\n",
    "                                        data_format='batches') # указываем формат - батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем словарь, чтобы сделать модель\n",
    "dictionary = artm.Dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем словарь по батчам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.gather(data_path='batches')\n",
    "dictionary.filter(class_id='@default_class',\n",
    "                  min_df=10, max_df=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его можно сохранить, чтобы не создавать снова или чтобы посмотреть и подредактировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_text('dict.txt')\n",
    "# dictionary = artm.Dictionary()\n",
    "# dictionary.load_text('dict.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель и сразу включаем два регуляризатора (Декореляции и Сглаживания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`artm.DecorrelatorPhiRegularizer` - регуляризатор декорреляции тем, делает темы менее похожими друг на друга (рекомендуется включать его сразу на всех темах с положительным значением tau и не менять его в процессе обучения)  \n",
    "\n",
    "`artm.SmoothSparsePhiRegularizer` - регуляризатор сглаживания/разреженивания Phi (тем) - сглаживает или расреживает распределение слов в темах (отрицательный tau - разреживает, положительный - сглаживает), рекомендуется сразу включать сглаживание на всех темах и не менять его в процессе обучения;\n",
    "\n",
    "Количество тем нужно подбирать, но 200 часто работает хорошо. Снижение количества тем и уменьшения словаря ведут к уменьшению времени обучения модели (а увеличение, наоборот). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artm = artm.ARTM(topic_names=['topic_{}'.format(i) for i in range(200)],\n",
    "                       \n",
    "                       regularizers=[\n",
    "                           artm.DecorrelatorPhiRegularizer(\n",
    "                                            name='Decorr', tau=0.25, \n",
    "                                            class_ids=['@default_class'],\n",
    "                                            topic_names=['topic_{}'.format(i) for i in range(1, 200)]),\n",
    "                           \n",
    "                           artm.SmoothSparsePhiRegularizer(\n",
    "                                            name='SmoothPhi_1', \n",
    "                                            class_ids=['@default_class'],\n",
    "                                            tau=0.15, \n",
    "                                            topic_names=['topic_{}'.format(i) for i in range(0, 200)])\n",
    "                                    ]\n",
    "                        )\n",
    "\n",
    "# не забывайте менять количество топиков в регуляризаторах, они применяются только на тех темах, что заданы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализурем модель словарем\n",
    "model_artm.initialize(dictionary=dictionary, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем метрики\n",
    "model_artm.scores.add(artm.PerplexityScore(name='PerplexityScore',\n",
    "                                                    dictionary=dictionary))\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore')) # разреженность слов в темах\n",
    "model_artm.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore')) # разреженность тем в доках\n",
    "model_artm.scores.add(artm.TopicKernelScore(name='TopicKernelScore', probability_mass_threshold=0.1)) # когерентность по семантичесим ядрам\n",
    "model_artm.scores.add(artm.TopTokensScore(class_id='@default_class', name='TopTokensScore_1', num_tokens=10)) # топ-n-слов для каждой темы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдемся несколько раз по коллекции, чтобы модель немного сошлась (если доков сильно больше то может быть достаточно и 1 прохода). Для сильно больших коллекций есть fit_online, который обновляется в процессе прохода по коллекции, про него можно почитать в документации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artm.num_document_passes = 2\n",
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, что стало с моделью\n",
    "# если график сильно падал на последних итерациях, то можно прогнать обучение ещё несколько раз \n",
    "# (не добавляя другие регуляризаторы)\n",
    "print_measures(model_artm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим регуляризаторы разреженности тем и документов.\n",
    "\n",
    "Их рекомендуется добавлять после того, как модель уже как-то сошлась. После добавления их рекомендуется постепенно уменьшать, чтобы усиливать разреживание. Их можно применять не ко всем темам, чтобы в каких-то темах собрались все стоп-слова.\n",
    "\n",
    "Можно добавлять их вместе или по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=-0.15, \n",
    "                                                            topic_names=['topic_{}'.format(i) for i in range(1, 200)],\n",
    "                                                            ))\n",
    "model_artm.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.15, \n",
    "                                                              topic_names=['topic_{}'.format(i) for i in range(1, 200)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer,num_collection_passes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# будем постепенно увеличивать значения этих регуляризаторов\n",
    "phi_tau = model_artm.regularizers['SparsePhi'].tau\n",
    "theta_tau = model_artm.regularizers['SparseTheta'].tau\n",
    "\n",
    "for i in range(10):\n",
    "    model_artm.regularizers['SparsePhi'].tau = (phi_tau + (phi_tau*0.5))\n",
    "    model_artm.regularizers['SparseTheta'].tau = (theta_tau + (theta_tau*0.5))\n",
    "    \n",
    "    model_artm.fit_offline(batch_vectorizer=batch_vectorizer,num_collection_passes=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на качество\n",
    "print_measures(model_artm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перплексия должна снижаться. Если график выровнился и больше не меняется - модель сошлась. Обычно перплексия хорошей модели около 200-1000. На таком небольшом количестве данных возможно такого значения достичь не получится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на темы\n",
    "for topic_name in model_artm.topic_names[:50]:\n",
    "    print(topic_name + ': ')\n",
    "    try:\n",
    "        for x in model_artm.score_tracker['TopTokensScore_1'].last_tokens[topic_name]:\n",
    "            print(x)\n",
    "        print('---------')\n",
    "    \n",
    "    except KeyError: # можно перекрутить параметры и некоторые темы окажутся пустыми\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основаная задача - **построить хорошую тематическую модель с интерпретируемыми топиками**.\n",
    "\n",
    "1) сделайте нормализацию (если pymorphy2 работает долго используйте mystem или попробуйте установить быструю версию - `pip install pymorphy2[fast]`, можно использовать какой-то другой токенизатор) \n",
    "\n",
    "2) добавьте нграммы (в тетрадке есть закомменченая ячейка с Phrases); \n",
    "\n",
    "3) сохраните тексты .vw формате;\n",
    "\n",
    "4) сделайте хороший словарь (отфильтруйте слишком частотные и редкие слова, попробуйте удалить стоп-слова, сохраните словарь и посмотрите на него, вдруг что-то плохое сразу будет заметно); \n",
    "\n",
    "5) постройте несколько ARTM моделей (переберите количество тем, поменяйте значения tau у регуляризаторов), если получаются плохие темы, поработайте дополнительно над предобработкой и словарем; \n",
    "\n",
    "6) для самой хорошей модели в отдельной ячейке напечатайте 3 хороших (на ваш вкус) темы\n",
    "\n",
    "7) в другой ячейки нарисуйте график обучения этой модели \n",
    "\n",
    "8) в третьей ячейки опишите какие параметры (количество тем, регуляризаторы, их tau) вы использовали и как обучали (например, после скольки проходов добавили регуляризатор разрежнивания тем (Phi), добавляли ли разреженность документам (Theta) и когда, как повышали значения, сколько итерации модель продожала улучшаться (снижалась перплексия, росли другие метрики);\n",
    "\n",
    "Сохраните тетрадку с экспериментами и положите её на гитхаб, ссылку на неё укажите в форме.\n",
    "\n",
    "**Оцениваться будут главным образом пункты 6, 7 и 8. (3, 1, 4 баллов соответственно). Чтобы заработать остальные 2 балла, нужно хотя бы немного изменить мой код на промежуточных этапах (добавить что-то, указать другие параметры и т.д). **"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
